如何解释昨天晚上做梦梦到了自己在训练一个神经网络的时候发现自己少加上了一个卷积核为3*3的Conv 层，但是最后的效果一摸一样

# CS230 C4W2-5: 经典卷积神经网络设计及CNN常见应用

之前我们探讨了什么是CNN，CNN的基本架构，但是设计CNN取得最好的效果也非常重要。经典的CNN设计可以给我们提供这种设计的思路，而当我们知道如何设计一个CNN之后，在落实到具体的项目（人脸识别，图像生成，物体识别）时又分别有些独特的地方，下面将会一一进行介绍

## 经典CNN设计

我们的CNN所需要调整的超参数主要有：卷积核的大小，channel数量，连接关系。而在经典的CNN则告诉我们

- 卷积核大小&channel 数量:对于相邻层之间保持整体的信息量不变(LeNets5,AlexNet)
- 通过池化层来调整整体的信息量，并且比例大概为1/2左右(VGG-16)
- 如何进行串联（ResNets）
- 1*1卷积核以及由此引发的并联讨论(GoogleNets)

对于LeNets5,AlexNets, VGG-16都是比较早期的网络，如果我们将一个CNN看成是一堆电阻的连接的话，那么这三者就是研究每一个电阻的设计的。

### LeNets5

废话少说，直接上网络图



如果我们想要计算它的参数的话，可以发现，对于前面卷积层的参数是非常少的，对于一个有c个channels的，卷积核$$r*r$$的卷积层，参数大小只有$$r*r*c$$,但是全连接层的参数量可以说是非常的大。于是**对于全连接层，我们一般会下降到几百几千的时候再使用FC**



同时LeNets的设计使用了**Conv=>Pool=>Conv=>Pool...**的结构，Pool可以认为是整合信息，而Conv是提取信息，对于两者交替进行的设计也是非常重要的想法。

LeNets5：规定了CNN的整体结构，Conv&Pool交替+最后的FC

## AlexNets

AlexNets在CNN设计上没有特别突出的地方，但它是真正意义上的第一个深度神经网络，总共有60M的参数，总共10几层，可以在ImageNets上实现1000种图片分类正确率80%的效果。在15年的时候是非常牛B的。

AlexNets的关键不止于此。往年的神经网络基本都是使用sigmoid&tanh(比较符合人脑神经元的激活条件)，但是AlexNets为了训练更深的网络，使用了非常多的**ReLU**函数，同时它还是第一个使用多个GPU进行训练的神经网络，就凭以上几点，AlexNets就以及非常NB了

AlexNets: ReLU,深度NN,GPU training

### VGG16



VGG-16也是一个非常大的网络，在人脸识别种也有比较广泛的应用。它的创新点使用padding这个东西，卷积层不改变图像大小，只在池化层将整体大小缩小一倍。同时它还提出了使用小卷积核来代替大卷积核的想法。

之前的LeNets之类的都使用了很多$$5*5,7*7$$的卷积核，但是VGG16却告诉我们使用$$3*3$$的卷积核也可以，不仅增加网络的深度，同时也减少了参数量。同时它还告诉了我们对于整体信息而言，需要保持以信息量的观点去设计网络

小卷积核使得提取信息从小到大均可，这增加了网络的可迁移性，为之后的 Transform Learning提供了基础

以上都是卷积核&通道的设计思路，但是对于更深的网络，又该怎么办呢？以上网络都是一条路走到黑，如果我们想要将网路串联起来怎么办呢，ResNets以及GoogleNets就解答了这个问题

### ResNets

