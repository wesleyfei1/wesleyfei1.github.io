# 🎓 人工智能 + 强化学习 系统学习计划（16周）

## 🧠 总体目标

- 构建 AI + 深度学习 + 强化学习 的完整理论与实践基础
- 注重代码能力与数学推导并进
- 逐步迈入强化学习方向的理论研究与论文复现

---

## 📚 学习模块与任务分类

| 模块 | 子任务 | 学习目标 | 学习材料 |
|------|--------|-----------|-----------|
| 📘 机器学习理论 | - 统计学习方法<br>- 模型评估、泛化能力 | 掌握经典 ML 模型与泛化理论 | 《统计学习方法》<br>吴恩达课程 |
| 📘 深度学习 | - 神经网络结构<br>- 优化算法<br>- 框架实战 | 理解 DL 核心算法 + PyTorch 熟练 | 《动手学深度学习》<br>Goodfellow《Deep Learning》 |
| 🤖 强化学习基础 | - MDP<br>- Policy Evaluation<br>- Q-Learning | 熟悉 RL 的数学模型与基础算法 | Sutton《Reinforcement Learning》 |
| 🔬 深度强化学习 | - DQN、PG、Actor-Critic<br>- Exploration/Exploitation | 掌握主流 RL 算法，具备实战能力 | OpenAI Spinning Up<br>CS285、David Silver |
| 📚 强化学习理论研究 | - 收敛性、泛化性<br>- PAC-MDP、Bandit | 走向 RL 研究，具备论文阅读与分析能力 | 《Bandit Algorithms》<br>最新论文（arXiv） |

---

## 🗓️ 每周计划表（16 周）

### 🧩 第1阶段：ML + 深度学习基础（第1~4周）

| 周 | 模块 | 学习内容 | 输出任务 |
|----|------|-----------|-----------|
| 第1周 | 机器学习 | 吴恩达课前4周内容 + 《统计学习方法》前2章 | 完成逻辑回归实现 |
| 第2周 | 机器学习 | 决策树、SVM、模型评估 | 使用 `sklearn` 完成实验 |
| 第3周 | 深度学习 | 动手学 DL 1~3章，PyTorch 基础 | 线性分类模型实现 |
| 第4周 | 深度学习 | MLP + Dropout + 正则化 | MNIST 分类 PyTorch 实现 |

---

### 🧩 第2阶段：强化学习基础 + 深度强化学习入门（第5~8周）

| 周 | 模块 | 学习内容 | 输出任务 |
|----|------|-----------|-----------|
| 第5周 | 强化学习 | Sutton RL 1~4章，理解 MDP、策略 | GridWorld 下的 DP 算法实现 |
| 第6周 | 强化学习 | Q-learning、Monte Carlo 方法 | 实现 CliffWalking 的 Q-learning |
| 第7周 | 深度 RL | DQN 理论与实践 | 用 PyTorch 实现 DQN 训练 CartPole |
| 第8周 | 深度 RL | REINFORCE, A2C 理解与实现 | PyTorch 实现 PG 方法 |

---

### 🧩 第3阶段：强化学习进阶 + 理论研究（第9~12周）

| 周 | 模块 | 学习内容 | 输出任务 |
|----|------|-----------|-----------|
| 第9周 | 深度学习 | CNN、RNN、优化器 | 完成 CIFAR-10 或文本任务 |
| 第10周 | 深度 RL | PPO 理论 + 论文阅读 | 实现 PPO 训练 LunarLander |
| 第11周 | RL 理论 | Bandit, Exploration 理论 | 推导 UCB 策略，写总结 |
| 第12周 | RL 理论 | PAC-MDP、泛化性论文阅读 | 阅读笔记 + 推导练习 |

---

### 🧩 第4阶段：论文复现 + 项目整合（第13~16周）

| 周 | 模块 | 学习内容 | 输出任务 |
|----|------|-----------|-----------|
| 第13周 | 项目实战 | 选择一篇 RL 论文进行复现 | 项目仓库初始化 |
| 第14周 | 项目实战 | 实验训练 + 复现验证 | 完成复现实验并文档整理 |
| 第15周 | 综合复盘 | 理论回顾 + 思维导图构建 | 输出 RL 学习总结 |
| 第16周 | 自由探索 | 阅读 2024/2025 RL 新论文 | 记录新方向备选清单 |

---

## 📁 学习输出建议

- 每周输出：
  - ✅ 学习笔记（推导、公式、算法流程）
  - ✅ 代码实现（PyTorch、Colab）
  - ✅ GitHub 项目文档（README + 结果分析）
- 每阶段总结：
  - 🧠 思维导图（如 RL 算法谱系图）
  - 📊 对比分析图（方法优劣 + 收敛性）
  - 📝 总结文章（可发布于博客/知乎）

---

## 🧰 工具推荐

| 工具 | 用途 |
|------|------|
| Obsidian / Notion | 笔记、计划整理 |
| GitHub + Jupyter + Colab | 实验管理 |
| Papers with Code + Arxiv | 阅读与追踪论文 |
| Weights & Biases | 实验可视化（可选） |

---

## 🧭 最终目标

- 理论上：能独立阅读强化学习前沿论文，理解算法收敛性、泛化性问题
- 实践上：能从 0 实现 DQN / PPO / SAC 等主流算法，撰写完整实验报告
- 研究上：具备开展 RL 理论相关方向的研究能力（如 PAC、Bandit、Offline RL 等）

---

