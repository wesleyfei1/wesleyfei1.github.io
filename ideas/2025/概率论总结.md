---
layout: page
permalink: /ideas/2025/概率论总结/index.html
title: 概率论总结
---
## 概率论
### Chap 1
1.对于样本空间S中发生的随机事件A会从$\omega1,\omega2,...\omega n$中的若干个$\omega i1,i2,...ik$中产生，对于每一个样本空间的元素称为样本点(基本事件)，而对于A的结果的集合$P(A)=\frac{|A|}{|S|}$称为A发生的概率
2.条件概率$P(A|B)=\frac{P(AB)}{P(B)}$表示已知事件B发生的情况下事件A发生的概率称为条件概率
- 全概率公式 若有$\Sigma Bi=\Omega,则P(A)=\Sigma P(Bi)P(A|Bi)$
- 贝叶斯公式 条件同全概率公式，有$P(Bi|A)=\frac{P(ABi)}{P(A)}=\frac{P(A|Bi)P(Bi)}{P(A)}$,可以实现对于A空间和S空间之间的转化，可以认为，全概率公式就是从A->|A|,而贝叶斯公式即为S->A
3.独立性：若AB是S中的两个事件，满足P(AB)=P(A)P(B),则称AB相互独立，同样的道理有大小为n的独立事件列，可以通俗的认为，独立事件列即为这些事件的发生互相之间没有影响
### Chap 2
1.随机变量，即为对于样本空间中的每一个样本点，都赋予一个值，记这样的映射为X,即为$X(\omega i)=这个样本点对应的值$，而随机变量发生的概率即为相应的值发生的概率
因此随机变量即为取值有一定的概率分布的变量
$P(X属于A)=P({\omega 其中X(\omega)属于A})$,可以认为就是一些数的取值的概率分布
2.离散型随机变量：P(X=xk)=pk,k=1......n,pk的和为1
3.连续型随机变量：
- 分布函数：设X为随机变量，x为任意实数，称F(x)=P(X<=x)为随机变量X的分布函数(从而对于离散型随机变量即可确定这个分布函数)
- 概率密度函数：设随机变量X的分布函数为F(x),若存在非负函数f(x)>=0,满足对于任意的x，$F(x)=\int^x_{-...}f(t)dt$称X为连续型随机变量，f(x)称为概率密度函数，即为X~f(x).
  因此对于P(X属于A)=$\int_A f(x)dx$,同时有F'(x0)=f(x)
4.随机变量函数的分布：
(1)对于离散型变量，若有Y=g(X),那么满足P(Y=g(xi))=pi，因此有概率分布P(Y=g(xk))=$\Sigma_{i:g(xi)=g(xk)}p_i$.
(2)连续型随机变量函数的分布
对于变换Y=g(X),从而Y的分布特点由X的分布以及g的性质决定，设X~f(x),以及f的分布函数为F0(x)
- 随机变量函数的分布函数：对于随机变量Y的分布函数F1(y)为
  $F_1 (y)=P(Y<=y)=P(g(X)<=y)=\int_{g(x)<=y}f(x)dx$
  **对于这种的求解对于单个的随机变量的变换的时候，基本的方法都是对于F'(Y)=f(x)在g(X)<=y下面的积分，而由此再对于y进行求导得到相应的f(x)**
- 密度变换公式
若g(x)严格单调而且反函数可导的时候，Y任然为连续型随机变量且
f1(y)=f(h'(y))|h'(y)|对于y再g(x)的值域里面，其中h(y)是g(x)的反函数
注:对于g不是再全部的区间上面单调的时候，可以考虑分成(a,b)=$\cup_j I_j$每一个子区间上面有相应的反函数hj(y),从而$f_1(y)=\Sigma_j f(h_j(y))|h'_j(y)|I_j$,但是对于一般的变换还是使用上面的方法比较好
### Chap 3
1.多维随机变量：可以理解为有X1,X2....Xn个随机变量，对于每一个变量之间相互独立？(依据S空间的基本事件而定)，n个随机变量共同组成了n维随机变量，称为$X(w)=(X_1(w),X_2(w).....X_n(w))$
- 从而相应的，有对应的分布函数：设(X,Y)为二维随机变量，(x,y)$\in R^2$,则$F(x,y)=P(X\leq x,Y\leq y)=P((w:X(w)\leq x,Y(w)\leq y))=P((X\leq x)\cap (Y\leq y))$称为(X,Y)的(联合)分布函数
'于是对于区域P((x1,x2)*(y1,y2))=F(x2,y2)-F(x2,y1)-F(x1,y2)+F(x1,y1)'
- 离散型随机变量分布律：对于X,Y可能取值(xi,yj)有$P(X=x_i,Y=y_j)=p_{ij}$，而对于其中的$P(X=x_i,Y=y_j)=P(X=x_i|Y=y_j)P(Y=y_j)$区进行计算(条件概率)
  同理还有其余维度的离散型随机变量
2.连续型多维随机变量的联合密度函数：
若(X,Y)~F(x,y)，对于任何的在$R^2$中的(x,y),有$F(x,y)=\int^x_{-\infty}\int^y_{-\infty} f(u,v)dudv $称f(x,y)为联合密度函数，
- 容易发现对于$P(x\in A)=\int\int_ Af(x,y)dxdy$
3.边缘分布
定义：设(X,Y)的联合分布函数为F(x,y),则其分量X,Y的分布函数F1(x),F2(y)称为F的边缘分布(即为对于x而言就是取x轴做一个竖直的切片)
- 二维离散型随机变量的边缘分布：
  $P(X=xi)=\Sigma^\infty_{j=1}P(X=xi,Y=yj)=\Sigma_j pij$
   $P(Y=yj)=\Sigma^\infty_{i=1}P(X=xi,Y=yj)=\Sigma_i pij$
通俗的来说就是将这一列所有的东西全部加起来，而只有在两个变量相互独立的时候才有P(X=xi,Y=yj)=P(X=xi)P(y=yj);
- 二维连续型随机变量的边缘分布
  由于$F_1(x)=F(x,\infty)=\int^x_{-\infty}\int^\infty_{-\infty}f(u,y)dudy$左右对于x求导既有$f_1(x)=\int^\infty_{-\infty}f(x,y)dy$,同理，有$f_2(y)=\int^\infty_{-\infty}f(x,y)dx$
  X和Y的概率密度函数f1(x),f2(y)称为二维随机变量(X,Y)的边缘概率密度函数，同理可以推广到高维
4.条件分布：
对于高维的随机变量的分布而言，还需要考察当一个量给定的时候其余的变量的分布关系
定义：如果Y的概率密度函数在y处的值f2(y)>0,称$f_{X|Y}(x|y)=\frac{f(x,y)}{f_2(y)}$,即为在给定Y=y下随机变量X的条件概率密度函数，同理，有$f_{Y|X}(y|x)=\frac{f(x,y)}{f_1(x)}$,记为Y|x~$f_{Y|X}(y|x)$(Y|X=x),在这个东西的基础之上可以加上贝叶斯公式的形式
同样的道理，还可以推广到更高的维度上面
5.相互独立的随机变量:设随机变量X,Y的联合分布为F(X,Y),边缘分布为F1(X),F2(y),如果对于任何的(x,y),有F(x,y)=F1(x)F2(y)则称X,Y相互独立
在(X,Y)为连续型随机变量的时候，等价于f(x,y)=f1(x)f2(y),而等价于f(x,y)可以分离变量=g1(x)g2(y),同理有多维的情况
6.随机向量函数的分布：(变换)
设(X,Y)~f(x,y),Z=g(X,Y)为一维随机变量则有$P(Z\in A)=\iint_{g(x,y)\in A} f(x,y)dxdy$
从而有Z的分布函数$Fz=P(Z\leq z)=\iint_{g(x,y)\leq z}f(x,y)dxdy$
- 若对于Z=(Z1,Z2)=(g1(X,Y),g2(X,Y)),$P((Z1,Z2)\in A)=\iint _{(g1(x,y),g2(x,y))\in A} f(x,y) dxdy$,同样的道理有相应的联合分布函数Fz(z1,z2);
- 若是对于密度函数而言，设u=g1(x,y),v=g2(x,y),则记相应的反函数$x=\varphi_1(u,v),y=\varphi_2(u,v)$,则可以计算得到相应的jaccobi行列式T=$\frac{\partial(x,y)}{\partial (u,v)}=(\frac{\partial(u,v)}{\partial (x,y)})^{-1}$,从而有
$F_Z(z1,z2)=\iint_{u\leq z1,v\leq z2}f(\varphi1(u,v),\varphi(u,v))Tdudv\to f_z(z1,z2)=f(\varphi1(u,v),\varphi(u,v))T|_{(u,v)=(z1,z2)}$
- **总结而言,对于变换的时候，还是先列出来Fz(z)的表达式，然后取进行计算，对于这个积分进行化简，而对于高维的时候，直接进行换元*
- **常见的结论：$f_{X+Y}(z)=f1*f2(z)$表示卷积**
### Chap 4
1.数学期望
- 离散型随机变量的期望：对于分布律P(X=xk)=pk,如果$\Sigma |xk|pk<\infty,E(X)=\Sigma_{k>0}xkpk$称为X的数学期望
- 连续型随机变量的期望：设X~f(x),如果E(|X|)存在，则有$E(X)=\int_Rxf(x)dx$为连续型随机变量的数学期望
- **期望的重要性质**
  1.$E(X1+X2+X3+...+Xn)=\Sigma E(Xi)$
  2.若X1,X2是互相独立的随机变量，则有E(X1X2)=E(X1)E(X2)
2.条件期望：
设E(|Y|)存在，称$E(Y|X=x)=\int ^\infty _{-\infty}yf_{Y|X}(y|x)dy$为给定了X=x时随机变量的条件期望，而这里面的E(Y|x)也可以看成时x的函数，即为h(x).
而对于高维的情况下面，我们无法写出对于某一个点的值，但是对于这种情况下面，我们可以以某一个元素为主，考虑其余的元素对于某一个值取定时候的平均值，从而可以判断对于高维下面的平均
以二维为例，由于已有h(x)=E(Y|x),$E(h(X))=E(E(Y|x))=\int ^\infty _{-\infty} h(x)f1(y)dx=\int_R yf_2(y) dy=E(Y)$
- 常见应用：$E(X)=\Sigma E(X|Y=i)P(Y=i)$
4.中位数与众数
- 中位数：设随机变量X~F(x),若存在常熟m有P(X>=m)=1-F(m)>=0.5,F(m)>=0.5,则称常数m为随机变量X的中位数
- 众数:离散型随机变量概率质量函数最大值对应的随机变量取值即为众数md，连续型随机变量就是f(x)到达最大值的那个x称为时众数，
- p份位数：P(X<=Qp)>=p,P(X>=Qp)>=1-p;


6.重要分布以及函数以及其期望，均值
>以下是一些有离散型变量的相关分布
- 0-1分布(伯努利分布) 若P(X=x)=p(x=1),1-p(x=0)
  E(X)=p,Var(X)=p(1-p)
- 离散均匀分布： P(X=xk)=1/n
- 二项分布：设X所有的可能的取值为0,1....n，若其分布律满足$P(X=k)=C^k_n p^k (1-p)^k$称X服从二项分布，即为X~B(n,p);
  几何含义为进行n次伯努利分布，X=X1+X2+X3+...+Xn.
  E(X)=np,Var(X)=np(1-p)
- 几何分布： 不断进行伯努利分布，知道第一次实验成功时所画的实验次数，称为几何分布,记为X~Ge(p)，有$P(X=k)=q^{k-1}p,q=1-p$
  $E(X)=\frac{1}{p},Var(X)=\frac{1-p}{p^2}$
- 负二项分布：将伯努利实验不断进行下去，知道第r次实验成功时的实验次数,$P(Xr=k)=C^{r-1}_{k-1}p^r q^(k-r)$,即为X~NB(r,p)
  几何意义就是对于进行r次的几何分布所花的总部书，即为X=X1+X2+...+Xr
- 泊松分布：可以近似认为时二项分布中在np不变的情况下面(类比于密度)
  在n无穷大时候的情况。即为$P(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}$,称X~P(\lambda)，
  - 泊松逼近定理：设Xn~B(n,pn),有$lim_{n\to \infty}P(X_n=k)=e^{-\lambda}\frac{\lambda^k}{k!}$,从而可以使用这个定理来进行估计
  $E(X)=\lambda,Var(X)=\lambda$
>连续型分布
- 均匀分布：随机变量X在有限区间(a,b)内取值，概率密度函数$f(x)=\frac{1}{b-a}I_{(a,b)}(x)$称X服从区间(a,b)上的均匀分布，记为X~U(a,b);//注意这里的I(a,b)(x)表示只有在a<x<b的时候这个东西是1，否则是0
  - E(x)=0.5(a+b),Var(x)=0;
  - 二维情况下(X,Y)~$f(x,y)=\frac{1}{|G|}I_G(x,y)$
- 指数分布：若随机变量X的密度函数满足$f(x)=\lambda e^{-\lambda x}I_{(0,\infty)}(x)$记为X~Exp(\lambda)
  - 性质：无记忆性：P(X>s+t|X>t)=P(X>s);
  - $E(X)=\frac{1}{\lambda},Var(X)=\frac{1}{\lambda^2}$
- 正态分布：
  - 定义：随机变量X密度函数$f(x)=\frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{2\sigma^2})$称X服从参数为mu,sigma2的正太分布，记为$X ~ N(\mu,\sigma^2)$
  - 对于N(0,1)称为标准正态分布，而$F(x)=\Phi(\frac{x-\mu}{\sigma}),之后，有f_0(x)=\frac{1}{\sqrt{{2\pi}}} exp(-\frac{x^2}{2})$称之为标准化变换
  - $E(x)=\mu,Var(x)=\sigma^2$
- 二维情况下的正态分布
  - 定义：设(X,Y)的概率密度函数为$f(x,y)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}exp\{-\frac{1}{2(1-\rho^2)}[\frac{(x-a)^2}{\sigma_1^2}-2\rho\frac{(x-a)(y-b)}{\sigma_1\sigma_2}+\frac{(y-b)^2}{\sigma_2^2}]\}$记为N(a,b,\sigma1^2,\sigma2^2,\rho)
  - 边缘分布：
    $F_1(x)=\int_R f(x,y)dy=N(a,\sigma_1^2)$,同理，有F2(y)的式子，即为随机变量X,Y都满足正态分布，而且都于\rho无关。
    条件分布：对于二维正太分布变量(X,Y)中
    Y|x~$N(\mu_2+\rho\sigma_2\frac{x-\mu_1}{\sigma_1},(1-\rho^2)\sigma_2^2)$

5.**方差与标准差**
设随机变量X是平方可积的，则$\sigma^2=Var(X)=E((X-\mu)^2),\sigma=\sqrt{Var(X)}$
- 方差性质
  (1)由期望的线性性，$Var(X)=E(X^2)-\mu^2$
  (2)常数$Var(c)=0,Var(cX)=c^2Var(X),Var(X+c)=Var(X)$，方差为0当且仅当所有的值相等
  (3)独立随机变量和的方差等于随机变量方差的和
  $Var(\Sigma X_i)=\Sigma Var(Xi)$(只有独立的时候才可以保证是交叉项是可以拆开来的)
  特别的，如果X1,X2,X3...Xn为分布相同的随机变量(i.i.d随机变量)，则对于$X=\frac{X1+X2+...+Xn}{n},ci=\frac{1}{n},Var(X)=Var(X_平)=\frac{\sigma^2}{n}$
- 矩
  - 设X为随机变量，满足$E(|X|^k)<\infty$则称$E((X-c)^k)$为X关于c的k阶矩，其中c为常数，称E(X^k)为k阶远点矩，$\mu_k=E((X-E(X))^k)$为X的k阶中心矩
  - $\mu_3$常常用来描述分布有没有篇，=0的时候没有偏，>0的时候式右偏,<0的时候是左偏，而我们用$\beta_1=\frac{\mu_3}{\sigma^3}$称为X的偏度系数来衡量，而$\mu_4,\beta_4$则可以描述图像的陡峭程度
  - X的**矩母函数**$M_X(s)=E[e^{sX}]$则可以发现$E(X^k)=\frac{d^k}{ds^k}M_X(s)|_{s=0}$,因此要称为是矩母函数
  注：MX(s)可以唯一确定随机变量X的分布
- 协方差
  - 定义：设随机变量X，Y均平方可积，则有Cov(X,Y)=E[(X-E(x))(Y-E(Y))]为X,Y的协方差
  - 相关性质
  (1)Cov(X,Y)=E(XY)-E(X)E(Y)
  (2)(线性性)Cov(aX+bY,cX+dY)=acVar(X)+bdVar(Y)+(ad+bc)Cov(X,Y)
  (3)当X，Y相互独立时Cov(X,Y)=0
  (4)[Cov(X,Y)] ^2 <=Var(X)Var(Y),当且仅当c1X+c2Y+c3=0时取等
  - 相关性质：由柯西施瓦兹不等式由$\rho_{X,Y}=Cov(\frac{X-E(X)}{\sqrt{Var(X)}},\frac{Y-E(Y)}{\sqrt{Var(Y)}})=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$为X,Y的相关系数。
    等于0时候线性无关，|t|->1表示相关性越好，而t>0正相关，反之为负相关
  - 对于二维的正态分布而言$Cov(X,Y)=\rho$,若X,Y互相独立<->p=0
6.熵
- 意义：熵度量了随机变量中所含有的星系两的大小，即为随机变量的不确定度，熵越大，随机变量越平均，反之越小
  定义：若X是离散型随机变量，则P(X=xk)=pl,熵为$H(X)=-\Sigma p_k log_2(p_k)$,
        若X是连续型随机变量，$H(X)=-\int^\infty_{-\infty}f_X(x)ln f_X(x)dx$
- 而在求解的时候使用变分，在g=0的时候取得最值，而将所有的约束E(x),Var(X),以及本身的概率的和为1作为一个限制条件，使用lagrange乘子法求解
  
7.大数定理于中心极限定理
对于中心极限定理是用于那些当N非常大的时候的Sn(X)的解的分布，这些分布收敛于正太分布，而对于大数定理，就是描述对于一个实验重复多次，他会越来越接近于某一个值
- 依概率收敛：设X1,X2,....Xn....是一随机变量序列，X为随机变量，若$\forall \epsilon>0,lim_{n\to \infty} P(|X_n-X|\geq \epsilon)=0$，则称随机变量序列{Xn}依概率收敛于随机变量X
  - 理解：我们可以先考虑是对于X是一个常数p，对于Xi的期望是p,而对于Pn在n越来越大的时候会与p的差过大的概率越来越小，于是可以认为到最后收敛于p，而这里的化就是对于事件An={|Xn(w)-X|>=\epsilon}发生的概率pn由lim pn=0,那么可以认为是Xn->X
- 马尔可夫不等式：若随机变量$Y\geq 0，\forall \epsilon>0,P(Y\geq \epsilon)\leq \frac{E(Y)}{\epsilon}$
  推论：切比雪夫不等式：$P(|X-\mu|\geq \epsilon)\leq \frac{Var(X)}{\epsilon^2}$
- **大数定律**：设X1,X2,...Xn...是一个i.i.d.随机变量序列，记他们相同的期望,方差为$\mu,\sigma^2,S_n=X_1+X_2+...+X_n,则对于\forall \epsilon>0,lim_{n\to \infty}P(|\frac{S_n}{n}-\mu|\geq \epsilon)=0$
   解释就是：对于$n\to \infty$的时候，X的密度函数几乎变成了一条线，即为对于这个东西会越来越趋近于中间值，从而可以用样本均值估计总体均值
- 依分布收敛：设X1,X2...Xn...为一系列随机变量，X为随机变量，Fn和F为随机变量Xn和X的分布函数，若对于F的所有连续点x,有lim Fn(x)=F(x),称Xn依分布收敛
  定理：依概率收敛->依分布收敛，而只有在X收敛于一个数c的时候才会有分布->概率
- **林德伯格-莱维中心极限定理**
  设X1,X2,...Xn...为一系列i.i.d.随机变量序列，有相同的期望和方差$\mu,\sigma^2,Sn=\Sigma X_i,\forall x \in \R,lim_{n\to \infty}P(\frac{\sqrt{n}(S_n/n-\mu)}{\sigma}\leq x)=\Phi(x)$其中\Phi(x)为标准正态分布函数，即为Sn标准化后的分布函数近似于标准正态分布函数
  $lim_{n\to \infty}P(\frac{S_n-E(S_n)}{\sqrt{Var(S_n)}}\leq x)=\Phi(x)$
- 模里夫-拉普拉斯中心极限定理：
  - 而对于中心极限定理中的X取为二项分布表示在np比较大的时候可以使用正太分布来模拟二项分布
  - 若Sn=X1+...+Xn,Xi~B(1,p)(0-1分布)，则有
  $lim_{n\to \infty}P(\frac{S_n-np}{\sqrt{np(1-p)}}\leq x)=\Phi (x)$
  - 由于Sn~B(n,p)因此对于当n小的时候可以使用泊松分布，而n比较大的时候可以使用二项分布
    对于B(n,p)想要求出$P(X=k)=P(k-0.5\geq X\leq k+0.5)=\Phi(\frac{k+0.5-np}{\sqrt{np(1-p)}})-\Phi(\frac{k-0.5-np}{\sqrt{np(1-p)}})\\because\,of\,lagrange,we\, use\,f'\,ro\,replace\,f\\\approx\frac{1}{\sqrt{np(1-p)}}\varphi(\frac{k-np}{\sqrt{np(1-p)}})=\frac{1}{\sqrt{2\pi np(1-p)}}exp\{\frac{(k-np)^2}{2np(1-p)}\}$
    同理对于$P(k_1\geq X\leq k_2)=P(k_1-0.5\geq X\leq k_2+0.5)=\Phi(\frac{k_2+0.5-np}{\sqrt{np(1-p)}})-\Phi(\frac{k_1-0.5-np}{\sqrt{np(1-p)}})$从而既可以进行估计了